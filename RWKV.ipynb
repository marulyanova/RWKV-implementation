{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![schema.png](schema.png)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.nn import functional as F\n",
    "import math"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## R - receptance, K - key, V - value"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### блок WKV (Weighted Key Value) играет ключевую роль в механизме внимания, использует выходы из слоев Key и Value. Эти выходы затем используются для вычисления взвешенного суммарного значения (Weighted Sum)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![formula.png](formula.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![formula2.png](formula2.png\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![formula3.png](formula3.png)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [],
   "source": [
    "class RWKV_Time_Mixing(torch.nn.Module):\n",
    "    def __init__(self, embedding_dim, head_dim, num_heads, ctx_len):\n",
    "        super().__init__()\n",
    "        self.mu = nn.ReLU() # ??? Какая-то функция активации мю. ПОКА ЗАМЕНИЛА НА RELU\n",
    "        self.Receptance = nn.Linear(embedding_dim, num_heads * head_dim)\n",
    "        self.Key = nn.Linear(embedding_dim, num_heads * head_dim)\n",
    "        self.Value = nn.Linear(embedding_dim, num_heads * head_dim)\n",
    "        self.output = nn.Linear(num_heads * head_dim, embedding_dim)\n",
    "        \n",
    "        self.num_heads = num_heads # Кол-во голов\n",
    "        self.head_dim = head_dim # Размер головы\n",
    "        self.ctx_len = ctx_len\n",
    "\n",
    "        # Взяла с гита, сложна\n",
    "        with torch.no_grad(): # initial time_w curves for better convergence\n",
    "            ww = torch.ones(self.num_heads, self.ctx_len)\n",
    "            curve = torch.tensor([-(self.ctx_len - 1 - i) for i in range(self.ctx_len)]) # the distance\n",
    "            for h in range(self.num_heads):\n",
    "                if h < self.num_heads - 1:\n",
    "                    decay_speed = math.pow(self.ctx_len, -(h + 1)/(self.num_heads - 1))\n",
    "                else:\n",
    "                    decay_speed = 0.0\n",
    "                ww[h] = torch.exp(curve * decay_speed)\n",
    "                # print('layer', layer_id, 'head', h, 'decay_speed', round(decay_speed, 4), ww[h][:5].numpy(), '...', ww[h][-5:].numpy())\n",
    "        self.time_w = nn.Parameter(ww)\n",
    "        self.time_alpha = nn.Parameter(torch.ones(self.num_heads, 1, self.ctx_len))\n",
    "        self.time_beta = nn.Parameter(torch.ones(self.num_heads, self.ctx_len, 1))\n",
    "        self.time_gamma = nn.Parameter(torch.ones(self.ctx_len, 1))\n",
    "        self.time_shift = nn.ZeroPad2d((0, 0 , 1 , -1))\n",
    "\n",
    "    def forward(self, x):\n",
    "        B, T, C = x.size()\n",
    "\n",
    "        # Тоже взяла, сложна\n",
    "        TT = self.ctx_len\n",
    "        w = F.pad(self.time_w, (0, TT))\n",
    "        w = torch.tile(w, [TT])\n",
    "        w = w[:, :-TT].reshape(-1, TT, 2 * TT - 1)\n",
    "        w = w[:, :, TT-1:] # w is now a circulant matrix\n",
    "        w = w[:, :T, :T] * self.time_alpha[:, :, :T] * self.time_beta[:, :T, :]\n",
    "\n",
    "        x = torch.cat([self.time_shift(x[:, :, :C//2]), x[:, :, C//2:]], dim = -1)\n",
    "\n",
    "        #--------\n",
    "\n",
    "        x = self.mu(x) # (batch_size, seq_len, embedding_dim)\n",
    "        r = torch.sigmoid(self.Receptance(x)) # (batch_size, seq_len, head_dim)\n",
    "        k = self.Key(x) # (batch_size, seq_len, head_dim)\n",
    "        v = self.Value(x) # (batch_size, seq_len, head_dim)\n",
    "\n",
    "        # Из гитхаба\n",
    "        # k = torch.clamp(k, max=30, min=-60) # clamp extreme values. e^30 = 10^13\n",
    "        # k = torch.exp(k)\n",
    "        # sum_k = torch.cumsum(k, dim=1)\n",
    "        # kv = (k * v).view(B, T, self.n_head, self.head_size)\n",
    "        # wkv = (torch.einsum('htu,buhc->bthc', w, kv)).contiguous().view(B, T, -1)\n",
    "        # rwkv = torch.sigmoid(r) * wkv / sum_k\n",
    "        # rwkv = self.output(rwkv)\n",
    "\n",
    "        k = torch.exp(k) # Возводим Key в экспоненту для сумм в Attm+(W, K, V) ()\n",
    "        sum_k = torch.cumsum(k, dim = 1) # Кумулятивная сумма по , (batch_size, seq_len, head_dim)\n",
    "        kv = (k * v).view(B, T, self.num_heads, self.head_dim) # матричное уможение\n",
    "        wkv = (torch.einsum('htu,buhc->bthc', w, kv)).contiguous().view(B, T, -1) # непонятный какой-то этот ваш einsum\n",
    "        rwkv = wkv / sum_k # ну делим числитель на знаменатель\n",
    "        rwkv = self.output(rwkv)\n",
    "\n",
    "        return rwkv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [],
   "source": [
    "class RWKV_Channel_Mixing(torch.nn.Module):\n",
    "   def __init__(self, embedding_dim, hidden_dim):\n",
    "      super().__init__()\n",
    "      self.mu = nn.ReLU() # опять эта фигня. ПОКА ЗАМЕНИЛА НА RELU\n",
    "      self.Receptance = nn.Linear(embedding_dim, embedding_dim)\n",
    "      self.Key = nn.Linear(embedding_dim, hidden_dim)\n",
    "      self.Value = nn.Linear(hidden_dim, embedding_dim)\n",
    "\n",
    "   def forward(self, x):\n",
    "      B, T, C = x.size()\n",
    "      x = self.mu(x)\n",
    "      r = torch.sigmoid(self.Receptance(x))\n",
    "      kv = self.Value(self.Key(x))\n",
    "      return (r * kv).view(B, T, -1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [],
   "source": [
    "class RWKV_Block(nn.Module):\n",
    "    def __init__(self, embedding_dim, num_heads, head_dim, ctx_len, hidden_dim):\n",
    "        super().__init__()\n",
    "\n",
    "        self.Layer_Norm1 = nn.LayerNorm(embedding_dim)\n",
    "        self.Layer_Norm2 = nn.LayerNorm(embedding_dim)\n",
    "\n",
    "        self.timemix = RWKV_Time_Mixing(embedding_dim, head_dim, num_heads, ctx_len)\n",
    "        self.channelmix = RWKV_Channel_Mixing(embedding_dim, hidden_dim)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = x + self.timemix(self.Layer_Norm1(x))\n",
    "        x = x + self.channelmix(self.Layer_Norm2(x))\n",
    "    \n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [],
   "source": [
    "class RWKV_LM_Head(nn.Module):\n",
    "    def __init__(self, embedding_dim, n_class):\n",
    "        super().__init__()\n",
    "\n",
    "        self.Layer_Norm = nn.LayerNorm(embedding_dim)\n",
    "        self.output = nn.Linear(embedding_dim, n_class)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.output(self.Layer_Norm(x))\n",
    "    \n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [],
   "source": [
    "class RWKV_model(nn.Module):\n",
    "    def __init__(self, vocab_size, embedding_dim, num_heads, head_dim, ctx_len, hidden_dim, num_layers, n_class):\n",
    "        super().__init__()\n",
    "        # Объединить это всё надо\n",
    "        \n",
    "        self.embedding = nn.Embedding(vocab_size, embedding_dim)\n",
    "        self.Layer_Norm = nn.LayerNorm(embedding_dim)\n",
    "        self.blocks = nn.Sequential(*[RWKV_Block(embedding_dim, num_heads, head_dim, ctx_len, hidden_dim) for i in range(num_layers)])\n",
    "        self.lm_head = RWKV_LM_Head(embedding_dim, n_class)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.embedding(x)\n",
    "        x = self.Layer_Norm(x)\n",
    "        x = self.blocks(x)\n",
    "        x = self.lm_head(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Подготовка датасета"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<torch._C.Generator at 0x11ad09a90>"
      ]
     },
     "execution_count": 63,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# hyperparameters\n",
    "batch_size = 64\n",
    "block_size = 4\n",
    "max_iters = 5000\n",
    "eval_interval = 500\n",
    "learning_rate = 3e-4\n",
    "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "eval_iters = 200\n",
    "n_embd = 384\n",
    "n_head = 6 # D = 384 // 6\n",
    "n_layer = 6\n",
    "dropout = 0.2\n",
    "# ------------\n",
    "\n",
    "torch.manual_seed(1337)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--2024-05-27 15:34:37--  https://raw.githubusercontent.com/marulyanova/NLP_6sem/main/dataset_poetry_mac.txt\n",
      "Распознаётся raw.githubusercontent.com (raw.githubusercontent.com)… 185.199.110.133, 185.199.111.133, 185.199.108.133, ...\n",
      "Подключение к raw.githubusercontent.com (raw.githubusercontent.com)|185.199.110.133|:443... соединение установлено.\n",
      "HTTP-запрос отправлен. Ожидание ответа… 200 OK\n",
      "Длина: 677788 (662K) [text/plain]\n",
      "Сохранение в: «dataset_poetry_mac.txt.5»\n",
      "\n",
      "dataset_poetry_mac. 100%[===================>] 661,90K  4,10MB/s    за 0,2s    \n",
      "\n",
      "2024-05-27 15:34:37 (4,10 MB/s) - «dataset_poetry_mac.txt.5» сохранён [677788/677788]\n",
      "\n"
     ]
    }
   ],
   "source": [
    "!wget https://raw.githubusercontent.com/marulyanova/NLP_6sem/main/dataset_poetry_mac.txt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "\n",
    "with open('/Users/maria/Documents/NLP6sem/Project RWKV/dataset_poetry_mac.txt', 'r', encoding = 'MACCYRILLIC') as file:\n",
    "    lines = file.readlines()\n",
    "\n",
    "new_lines = []\n",
    "for line in lines:\n",
    "\n",
    "    # убрать пробелы в начале строки, оставить только русские буквы, убрать строки, где только цифры (года написания стихов), названия стихов\n",
    "\n",
    "    line = line.lstrip()\n",
    "    line = re.sub(r'[a-zA-Z]', '', line)\n",
    "    if line.isdigit():\n",
    "        continue\n",
    "    if line.isupper():\n",
    "            line = '*\\n'\n",
    "    new_lines.append(line)\n",
    "\n",
    "with open('dataset_poetry_mac_modified.txt', 'w', encoding = 'MACCYRILLIC') as file:\n",
    "    file.writelines(new_lines)\n",
    "\n",
    "with open('dataset_poetry_mac_modified.txt', 'r', encoding = 'MACCYRILLIC') as f:\n",
    "    text = f.read()\n",
    "\n",
    "chars = sorted(list(set(text)))\n",
    "vocab_size = len(chars)\n",
    "stoi = { ch:i for i,ch in enumerate(chars) }\n",
    "itos = { i:ch for i,ch in enumerate(chars) }\n",
    "encode = lambda s: [stoi[c] for c in s]\n",
    "decode = lambda l: ''.join([itos[i] for i in l])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Ты опять упрекнула меня,',\n",
       " 'Что я с музой моей раздружился,',\n",
       " 'Что заботам текущего дня',\n",
       " 'И забавам его подчинился.',\n",
       " 'Для житейских расчетов и чар',\n",
       " 'Не расстался б я с музой моею,',\n",
       " 'Но бог весть, не погас ли тот дар,',\n",
       " 'Что, бывало, дружил меня с нею?',\n",
       " 'Но не брат еще людям поэт,',\n",
       " 'И тернист его путь, и непрочен,',\n",
       " 'Я умел не бояться клевет,',\n",
       " 'Не был ими я сам озабочен;',\n",
       " 'Но я знал, чье во мраке ночном',\n",
       " 'Надрывалося сердце с печали',\n",
       " 'И на чью они грудь упадали свинцом,',\n",
       " 'И кому они жизнь отравляли.',\n",
       " 'И пускай они мимо прошли,',\n",
       " 'Надо мною ходившие грозы,',\n",
       " 'Знаю я, чьи молитвы и слезы',\n",
       " 'Роковую стрелу отвели...',\n",
       " 'Да и время ушло,- я устал']"
      ]
     },
     "execution_count": 66,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "text[:600].split('\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "626213"
      ]
     },
     "execution_count": 67,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(text) # длина датасета 600к символов"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = torch.tensor(encode(text), dtype=torch.long)\n",
    "n = int(0.9 * len(data))\n",
    "train_data = data[:n]\n",
    "val_data = data[n:]\n",
    "\n",
    "def get_batch(split):\n",
    "    data = train_data if split == 'train' else val_data\n",
    "    ix = torch.randint(len(data) - block_size, (batch_size,))\n",
    "    x = torch.stack([data[i:i+block_size] for i in ix])\n",
    "    y = torch.stack([data[i+1:i+block_size+1] for i in ix])\n",
    "    x, y = x.to(device), y.to(device)\n",
    "    return x, y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [],
   "source": [
    "@torch.no_grad()\n",
    "def estimate_loss():\n",
    "    criterion = nn.CrossEntropyLoss()\n",
    "    out = {}\n",
    "    model.eval()\n",
    "    for split in ['train', 'val']:\n",
    "        losses = torch.zeros(eval_iters)\n",
    "        for k in range(eval_iters):\n",
    "            X, Y = get_batch(split)\n",
    "            logits = model(X)\n",
    "            B, T, C = logits.shape\n",
    "            logits = logits.view(B * T, C)\n",
    "            Y = Y.view(B * T)\n",
    "            loss = criterion(logits, Y)\n",
    "            losses[k] = loss.item()\n",
    "        out[split] = losses.mean()\n",
    "    model.train()\n",
    "    return out"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Модель"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "24.024361 M parameters\n"
     ]
    }
   ],
   "source": [
    "model = RWKV_model(vocab_size, n_embd, n_head, n_embd, block_size, n_embd, n_layer, vocab_size).to(device)\n",
    "print(sum(p.numel() for p in model.parameters())/1e6, 'M parameters')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "step 0: train loss 4.6921, val loss 4.6823\n"
     ]
    }
   ],
   "source": [
    "try:\n",
    "  optimizer = torch.optim.AdamW(model.parameters(), lr = learning_rate)\n",
    "  criterion = nn.CrossEntropyLoss()\n",
    "\n",
    "  for iter in range(max_iters):\n",
    "\n",
    "      if iter % eval_interval == 0 or iter == max_iters - 1:\n",
    "          losses = estimate_loss()\n",
    "          print(f\"step {iter}: train loss {losses['train']:.4f}, val loss {losses['val']:.4f}\")\n",
    "\n",
    "      xb, yb = get_batch('train')\n",
    "      logits = model(xb)\n",
    "      B, T, C = logits.shape\n",
    "      logits = logits.view(B * T, C)\n",
    "      yb = yb.view(B * T)\n",
    "      loss = criterion(logits, yb)\n",
    "      optimizer.zero_grad(set_to_none = True)\n",
    "      loss.backward()\n",
    "      optimizer.step()\n",
    "\n",
    "except KeyboardInterrupt: 0"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Пример CUMsum по разным осям"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor([[[ 1,  2,  3],\n",
       "          [ 5,  7,  9]],\n",
       " \n",
       "         [[ 7,  8,  9],\n",
       "          [17, 19, 21]]]),\n",
       " torch.Size([2, 2, 3]))"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Вторая размерность (1) - сложение по i-му элементу по второму измерению))\n",
    "a = torch.tensor([[[1, 2, 3], [4, 5, 6]], [[7, 8, 9], [10, 11, 12]]]) \n",
    "torch.cumsum(a, dim = 1), torch.cumsum(a, dim = 1).shape\n",
    "\n",
    "# У нас в модели вот так, т.е. сложение по seq_len элементов head_dim (выходы складываются друг с другом по i-му эл-ту, покоординатно)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor([[[ 1,  2,  3],\n",
       "          [ 4,  5,  6]],\n",
       " \n",
       "         [[ 8, 10, 12],\n",
       "          [14, 16, 18]]]),\n",
       " torch.Size([2, 2, 3]))"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Первая размерность (0) - сложение по i-му элементу по первому измерению\n",
    "a = torch.tensor([[[1, 2, 3], [4, 5, 6]], [[7, 8, 9], [10, 11, 12]]]) \n",
    "torch.cumsum(a, dim = 0), torch.cumsum(a, dim = 0).shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor([[[ 1,  3,  6],\n",
       "          [ 4,  9, 15]],\n",
       " \n",
       "         [[ 7, 15, 24],\n",
       "          [10, 21, 33]]]),\n",
       " torch.Size([2, 2, 3]))"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Самая последняя размерность (2) - просто по порядку складываем чиселки.\n",
    "a = torch.tensor([[[1, 2, 3], [4, 5, 6]], [[7, 8, 9], [10, 11, 12]]]) \n",
    "torch.cumsum(a, dim = 2), torch.cumsum(a, dim = 2).shape"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
